{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e3773",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf372c85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, TextIteratorStreamer\n",
    "import torch\n",
    "import gradio as gr\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb36ba4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hf_token = userdata.get('HF_Token')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473448e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the instruct model names\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c17fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# define helper function to load model and tokenizer\n",
    "def load_model(model_name):\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "      bnb_4bit_quant_type=\"nf4\"\n",
    "  )\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=quant_config)\n",
    "  return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2571dd",
   "metadata": {},
   "source": [
    "# Different Types of Response Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bcc90a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# define different generating functions:\n",
    "#   1- full response\n",
    "#   2- low level streaming response\n",
    "#   3- high level streaming response\n",
    "\n",
    "def generate_full(tokenizer, model, user_input, max_tokens=2000):\n",
    "  global messages\n",
    "  # Append the user's new message to the conversation history\n",
    "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "  outputs = model.generate(inputs, max_new_tokens=max_tokens)\n",
    "  response = tokenizer.decode(outputs[0])\n",
    "  print(response)\n",
    "\n",
    "def generate_stream_low_level(tokenizer, model, user_input, max_tokens=2000):\n",
    "    global messages\n",
    "    # Append the user's new message to the conversation history\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Prepare the initial input\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "\n",
    "    # Generate up to 2000 tokens\n",
    "    for _ in range(max_tokens):\n",
    "        outputs = model(input_ids)  # Get the model's output (logits) for the given input IDs\n",
    "        # Select the token with the highest probability from the last position's logits\n",
    "        next_token_id = outputs.logits[:, -1].argmax(dim=-1).unsqueeze(-1)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)  # Append new token\n",
    "        next_token = tokenizer.decode(next_token_id[0])  # Decode and print\n",
    "        # flush=True ensures the output is immediately written to the console.\n",
    "        # By default, print output is buffered, so it may not appear instantly.\n",
    "        # flush=True forces the buffer to flush, making real-time output possible.\n",
    "        print(next_token, end=\"\", flush=True)\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:  # Stop if EOS token\n",
    "            break\n",
    "    print()\n",
    "\n",
    "def generate_stream_high_level(tokenizer, model, user_input, max_tokens=2000):\n",
    "  global messages\n",
    "  # Append the user's new message to the conversation history\n",
    "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "  # we skip using TextStreamer() here cause it streams back results to stdout and thats not what we want in gradio app\n",
    "  # and we use TextIteratorStreamer() instead\n",
    "\n",
    "  # Initialize the TextIteratorStreamer for streaming output\n",
    "  streamer = TextIteratorStreamer(\n",
    "      tokenizer,\n",
    "      skip_prompt=True,\n",
    "      decode_kwargs={\"skip_special_tokens\": True}\n",
    "  )\n",
    "\n",
    "  # Run the generation process in a separate thread\n",
    "  thread = threading.Thread(\n",
    "      target=model.generate,\n",
    "      kwargs={\"inputs\": inputs, \"max_new_tokens\": max_tokens, \"streamer\": streamer}\n",
    "  )\n",
    "  thread.start()\n",
    "\n",
    "  # Stream and print the output progressively\n",
    "  for text_chunk in streamer:\n",
    "    filtered_chunk = text_chunk.replace(\"<|eot_id|>\", \"\")  # Remove special tokens if present\n",
    "    print(filtered_chunk, end=\"\")  # Print without adding new lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce553669",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# call the helper function and load the model and tokenizer\n",
    "tokenizer, model = load_model(LLAMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce63a3",
   "metadata": {},
   "source": [
    "### Testing the three generating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5375b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the messages history, the max tokens for the model, and the user_input\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}\n",
    "]\n",
    "\n",
    "max_tokens = 2000\n",
    "\n",
    "user_input = \"What is the meaning of life? Answer in markdown and in 5 lines maximum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f961a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "generate_full(tokenizer, model, user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806760b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "generate_stream_low_level(tokenizer, model, user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dcc479",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "generate_stream_high_level(tokenizer, model, user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35aa657",
   "metadata": {},
   "source": [
    "# Adding a Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ff5f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# define the streaming function for gradio (using yield)\n",
    "def generate_stream(user_input):\n",
    "    # Global variables for modifications\n",
    "    global tokenizer, model, messages, max_tokens\n",
    "\n",
    "    # Step 1: Append the user's new message to the conversation history\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Step 2: Tokenize the input messages and convert them into a tensor\n",
    "    # - apply_chat_template: Formats the messages according to the model's expected input format.\n",
    "    # - return_tensors=\"pt\": Returns the result as a PyTorch tensor.\n",
    "    # - add_generation_prompt=True: Adds a special prompt or token for generation.\n",
    "    # - .to(\"cuda\"): Moves the tensor to the GPU for faster computation.\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "    # Type: torch.Tensor of shape [batch_size, sequence_length].to(\"cuda\")\n",
    "\n",
    "    # Initialize an empty string to accumulate the generated result\n",
    "    result = \"\"\n",
    "\n",
    "    # Step 3: Start generating tokens in a loop, up to a maximum of 2000 tokens\n",
    "    for _ in range(max_tokens):\n",
    "        # Step 4: Pass the current input sequence to the model to predict the next token\n",
    "        # - outputs.logits: Contains the raw prediction scores (logits) for all possible tokens.\n",
    "        # - Shape of outputs.logits: [batch_size, sequence_length, vocab_size].\n",
    "        # - outputs.logits[:, -1]: Selects the logits for the last token position (shape: [batch_size, vocab_size]).\n",
    "        outputs = model(input_ids)\n",
    "        # Type: transformers.modeling_outputs.CausalLMOutputWithPast containing logits of shape [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "        # Step 5: Find the token ID with the highest score (greedy decoding)\n",
    "        # - argmax(dim=-1): Selects the index of the maximum value along the vocab_size dimension.\n",
    "        # - unsqueeze(-1): Adds a new dimension at the last position, resulting in a shape of [batch_size, 1].\n",
    "        next_token_id = outputs.logits[:, -1].argmax(dim=-1).unsqueeze(-1)\n",
    "        # Type: torch.Tensor of shape [batch_size, 1].unsqueeze(-1)\n",
    "\n",
    "        # Step 6: Append the newly generated token ID to the input_ids tensor\n",
    "        # - torch.cat(): Concatenates the current input_ids with the next_token_id along the last dimension.\n",
    "        # - This updates input_ids to include the newly generated token, so the model can use the updated sequence in the next iteration.\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "        # Type: torch.Tensor of shape [batch_size, updated_sequence_length]\n",
    "\n",
    "        # Step 7: Decode the newly generated token ID into a human-readable string\n",
    "        # - tokenizer.decode(): Converts the token ID into its corresponding string.\n",
    "        # - skip_special_tokens=True: Ensures special tokens like <eos> (end-of-sequence) are not included in the output.\n",
    "        next_token = tokenizer.decode(next_token_id[0], skip_special_tokens=True)\n",
    "        # Type: str representing the decoded token\n",
    "\n",
    "        # Step 8: Accumulate the decoded token into the result string\n",
    "        result += next_token\n",
    "\n",
    "        # Step 9: Yield the accumulated result for streaming output\n",
    "        # - yield allows the function to return partial results without stopping, enabling real-time streaming.\n",
    "        yield result\n",
    "\n",
    "        # Step 10: Check if the model predicted the end-of-sequence (EOS) token\n",
    "        # - tokenizer.eos_token_id: The special token ID representing EOS.\n",
    "        # - If EOS is detected, break the loop to stop further generation.\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Append the final assistant response to the conversation history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d3ad3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# optimize the streaming function for gradio (using TextIteratorStreamer)\n",
    "def generate_stream_optimized(user_input):\n",
    "  # Global variables for modifications\n",
    "  global tokenizer, model, messages, max_tokens\n",
    "\n",
    "  # Step 1: Append the user's new message to the conversation history\n",
    "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "  # Step 2: Prepare the inputs for the model by applying the chat template\n",
    "  # The inputs include the conversation history and the user's latest message\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "  # we skip using TextStreamer() here cause it streams back results to stdout and thats not what we want in gradio app\n",
    "  # we use TextIteratorStreamer() instead\n",
    "\n",
    "  # Step 3: Initialize the TextIteratorStreamer\n",
    "  streamer = TextIteratorStreamer(\n",
    "      tokenizer,\n",
    "      skip_prompt=True,  # Ensures that the input prompt is not repeatedly included in the streamed output.\n",
    "      decode_kwargs={\"skip_special_tokens\": True}  # Filters out special tokens (e.g., <s>, </s>, <pad>, <cls>, <sep>) from the generated text.\n",
    "  )\n",
    "\n",
    "  # Step 4: Create a thread to run the generation process in the background\n",
    "  thread = threading.Thread(\n",
    "      target=model.generate,  # Specifies that the model's `generate` method will be run in the thread.\n",
    "      kwargs={                           # Passes the arguments required for text generation\n",
    "          \"inputs\": inputs,              # The tokenized input prompt for the model.\n",
    "          \"max_new_tokens\": max_tokens,  # Limits the number of tokens to be generated.\n",
    "          \"streamer\": streamer           # The TextIteratorStreamer to handle streaming the output.\n",
    "          }\n",
    "  )\n",
    "\n",
    "  # Step 5: Start the thread to begin the generation process\n",
    "  thread.start()\n",
    "\n",
    "  # Step 6: Initialize an empty string to accumulate the growing output\n",
    "  accumulated_reply = \"\"\n",
    "\n",
    "  # Step 7: Stream the output progressively\n",
    "  for text_chunk in streamer:  # Iterate over each chunk of text streamed by the model\n",
    "      # Filter out any unexpected special tokens manually if they appear to ensure a clean output\n",
    "      # `<|eot_id|>` is a special token (e.g., end-of-text marker) that may still appear in some outputs\n",
    "      filtered_chunk = text_chunk.replace(\"<|eot_id|>\", \"\")\n",
    "\n",
    "      # Append the filtered chunk to the accumulated text that holds all the generated text seen so far\n",
    "      accumulated_reply += filtered_chunk\n",
    "\n",
    "      # Yield the accumulated text to the calling function/UI for progressive updates,\n",
    "      # ensuring the output is continuously refreshed with new content\n",
    "      yield accumulated_reply\n",
    "\n",
    "  # Step 8: Append the final assistant response to the conversation history\n",
    "  messages.append({\"role\": \"assistant\", \"content\": accumulated_reply})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a8b6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Chat with AI (Streaming Enabled)\")\n",
    "    with gr.Row():\n",
    "      with gr.Column():\n",
    "        user_input = gr.Textbox(label=\"Your message\", placeholder=\"Type something...\")\n",
    "        output_box = gr.Markdown(label=\"AI Response\", min_height=50)\n",
    "        send_button = gr.Button(\"Send\")\n",
    "\n",
    "    send_button.click(fn=generate_stream_optimized, inputs=user_input, outputs=output_box)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
